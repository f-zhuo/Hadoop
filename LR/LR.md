# 线性回归

## 回归问题

给定一个输入值，预测输出值。比如给你一个输入信息，一个100平的房子，让你给出它的一个预测输出值：房价

那么问题来了，如何预测呢？这就需要建立一个数学模型h(x)，输入是房子的面积x，输出是房价y=h(x)。那么y和x的关系如何确定呢？设房子是θ万/平，易得$$y=h(x)=\theta x$$。这就是一个简单的可以描述两者关系的数学模型。这个模型是一个一元线性函数，称为假设函数，θ是一个常数，所以称之为线性回归（linear regression），只有一个自变量，又叫一元线性回归

![](.\pictures\liner regression.png)

但是，现实往往很残酷，房价还受到地段，品牌，市场等各方面的影响，真正能表示房价y的方程就变成了$$y=h(x)=\theta_{1}x_{1}+\theta_{2}x_{2}+...\theta_{n}x_{n}$$，其中每个θ都对应了一个x，表示其对x这个变量的影响，称为权重/回归系数，x称为特征

然而，上述的权重就真的能够描述房价了吗？还不够，除了上述的因素外，城市也是一个重要因素。上海的房价肯定要比拉萨高，注意，这里的城市因素不能归类为上述的x，因为上海的房价比拉萨高，体现在上述的每个方面。上海的繁华地段比拉萨的繁华地段房价高，上海的同等面积的房价比拉萨高。。。所以，城市这个因素体现的是对整体回归结果的影响，是一个客观存在，不是某一个方面。把这个因子称为偏置，记为b，则$$y=h(x)=\theta_{1}x_{1}+\theta_{2}x_{2}+...\theta_{n}x_{n}+b$$。事实上，当x都为0时，$$y=h(x)=b$$，也就是不考虑各个影响因子时，回归值就是偏置了。为了书写和求解方便，将h(x)写为：$$h(x)=\theta^{T}x$$，其中$$\theta^{T}=[\theta_{1},\theta_{2},...\theta_{n},b]$$，$$x^{T}=[x_{1},x_{2},...x_{n},1]$$，这被称为多元线性回归。若样本对(x,y)不止一个，$$x_{i}=[x^{1}_{i},x^{2}_{i},...x^{m}_{i}]$$，i表示第i个特征，m表示样本数量

如果已知θ和b，那么回归问题就相当简单，将x代入计算即可。但现实中模型往往是未知的，也就是说已知样本数据(x,y)，但θ和b未知。我们的工作就变成了求解θ和b，上面的方程将变为$$h_{\theta}(x)=\theta^{T}x$$，注意已知和未知量

怎么求θ和b呢？这里引入损失函数的概念

## 损失函数与回归系数求解

cost function，又叫代价函数，是一个描述模型的精度的概念，表示模型预测值和真实值的偏差情况。可以理解为可信度，模型越准确，预测值发生的可能性就越高，结果也越可信，损失函数就应该越小

这里采用最小二乘法描述损失函数，设预测值$$\hat h(x)$$为$$\hat{y}$$，损失函数为
$$
RSS=\sum_{i=1}^{m}(\hat{y}_{i}-y_{i})^{2}=\sum_{i=1}^{m}(\hat h_{i}(x)-y_{i})^{2}
$$
其中，m是样本对(x,y)数量
$$
\hat h_{i}(x)=\theta_{1}x_{1}^{i}+\theta_{2}x_{2}^{i}+...\theta_{n}x_{n}^{i}+b
$$
n是特征量，i属于[1,m]，代入可得
$$
RSS=\sum_{i=1}^{m}(\hat{y}_{i}-y_{i})^{2}=\sum_{i=1}^{m}(\theta_{1}x_{1}^{i}+\theta_{2}x_{2}^{i}+...\theta_{n}x_{n}^{i}+b-y_{i})^{2}
$$
上述已知，我们的目标是求得RSS的最小值，注意x，y是已知量，对θ偏导
$$
\frac{\partial RSS}{\partial \theta_{1}}=2\sum_{i=1}^{m}(\theta_{1}x_{1}^{i}+\theta_{2}x_{2}^{i}+...\theta_{n}x_{n}^{i}+b-y_{i})x^{i}_{1}=2\sum_{i=1}^{m}(\hat h_{i}(x)-y_{i})x^{i}_{1}\\
\frac{\partial RSS}{\partial \theta_{2}}=2\sum_{i=1}^{m}(\theta_{1}x_{1}^{i}+\theta_{2}x_{2}^{i}+...\theta_{n}x_{n}^{i}+b-y_{i})x^{i}_{2}=2\sum_{i=1}^{m}(\hat h_{i}(x)-y_{i})x^{i}_{2} \\...\\
\frac{\partial RSS}{\partial b}=2\sum_{i=1}^{m}(\theta_{1}x_{1}^{i}+\theta_{2}x_{2}^{i}+...\theta_{n}x_{n}^{i}+b-y_{i})=2\sum_{i=1}^{m}(\hat h_{i}(x)-y_{i})
$$
令上述各式为0，显然上述各式都是关于θ和b的二元一次线性方程，且方程的数量是n+1个，等于变量个数，是可解的

*h(x)函数是根据m组的(x,y)值人为拟合出来的，是假想的，所以并不存在θ满足所有的(x,y)。我们的任务是求出一组θ，使得代价函数最小，最大程度上满足(x,y)*

## 高次多项式回归

回到最开始的房价问题，这个模型真的精准吗？房子地段越好房价的确越高，但不可能满足线性方程，关系可能是高次的曲线

![](.\pictures\高次多项式回归.png)

如果$$h(x)=\theta x^{2}+b$$，那么还可以用线性回归解决吗？答案是肯定的，因为不管x取多少次幂，$$\frac{\partial RSS}{\partial \theta}=2\sum(\hat h(x)-y)x^{t}$$，这里t指h(x)中x的次幂，可以看出，只要把样本对(x,y)代入，这个梯度方程就一定是关于θ和b的二元一次方程，且方程个数就等于未知量，一定可解

事实上，把$$x^{2}$$看成x，高次多项式就退化为一次方程

# 逻辑回归

logistic regression，习惯称为LR。首先明确，逻辑回归虽然叫回归，但主要是来解决分类问题，尤其是二分类问题，叫做回归是因为LR基于线性回归做了“升级”

对于LR，因为要解决分类问题，我们就要做些规定方便计算。设第一类为正例，记为1，则第二类为负例，记为0。那么，LR的结果就是离散的，要么是0，要么是1，不可能像回归那样是连续值。所以，线性回归的模型函数不可用，$$h(x)=\theta x$$这个函数的值域是$$(-\infty,\infty) $$，显然也是不符合LR的要求的。阶跃函数可以满足要求
$$
h(\theta x)=\left\{\begin{matrix}
 1&\theta x>=0 \\ 
 0& \theta x<0
\end{matrix}\right.
$$
但显然，这个函数是不连续的，也不可导，就不可用（不管是上文的最小二乘法还是下文的梯度下降法，求解模型参数时需要求其导数）

所以，使用sigmoid函数来作为LR的模型：$$h(y=1|x)=1/(1+e^{-\theta x})=e^{\theta x}/(1+e^{\theta x})$$，则$$h(y=0|x)=1-h(y=0|x)=1-1/(1+e^{-\theta x})=1/(1+e^{\theta x})$$

<img src=".\pictures/sigmoid_function.png" style="zoom:70%" />

如何判断sigmoid的结果呢？可以设置一个阈值，大于阈值就认为是1，小于阈值就是0。比如设置0.5为阈值，那么结果正好为阈值的部分称为**决策边界**。使用sigmoid函数的好处是：

* 没有假设数据分布，直接对分类结果进行建模，避免了假设分布不准确带来的问题
* 不仅可以进行分类，还可以得到结果为该类的概率
* sigmoid函数是任意阶可导凸函数，方便求解

sigmoid函数的特点是，横坐标趋向负无穷时，纵坐标趋向于0；横坐标趋向于正无穷时，纵坐标趋向于1；横坐标为0时，纵坐标为0.5。可以看出sigmoid函数的值域在0,1之间，正好和概率值区间一致。我们可以认为，h(y=1|x)就是结果为1的概率，自然，h(y=0|x)就是结果为0的概率，将两者相除取对数
$$
ln\frac {h(y=1|x)}{h(y=0|x)}=lne^{\theta x}=\theta x
$$
右边就是熟悉的线性回归的假设函数。所以，LR是在线性回归的基础上又使用了sigmoid函数做映射，将原本的实数域的取值范围压缩到了0,1之间。同时，线性回归解决回归问题，目标是求得真实值的近似，输出连续值；LR解决分类问题，目标是求得为1/0的概率，求的是期望值，输出离散值

再回到决策边界的问题，既然是正例负例概率相等的部分，$$ln\frac{h(y=1|x)}{h(y=0|x)}=ln1=0$$，也就是$$\theta x=0$$，所以LR实际上是广义的线性回归

![](.\pictures\决策边界.png)

一般将LR的特征值离散化，让其稀疏，好处是：

* 特征值有大量的0存在，特征向量就是稀疏的，容易计算
* 离散化之后，再增加/删除特征很方便，不需要过多考虑区间划分
* 鲁棒性更强，对干扰项的适应性更好。比如房价，大于2万/平就记为1，哪怕10万/平也不会有过多影响，否则会干扰结果
* 特征离散化后模型更简单，降低了过拟合的可能
* 单个特征离散化为N个后，每个离散特征有单独的权重，相当于为模型引入了非线性，能够加大拟合

## 最大似然函数

如果某个参数使得结果出现的可能性最大，就把这个参数当作真实值的估计值

对于LR来说，每次结果是1的可能为h(y=1|x)，为0是h(y=0|x)，则对m个样本来说，结果为1的可能性是$$\prod_{i=1}^{m}(e^{\theta x_{i}}/(1+e^{\theta x_{i}}))$$，结果为0是$$\prod_{i=1}^{m}(1/(1+e^{\theta x_{i}}))$$，将两式合写为：
$$
L(\theta)=\prod_{i=1}^{m}(e^{\theta x_{i}}/(1+e^{\theta x_{i}}))^{y_{i}}(1/(1+e^{\theta x_{i}}))^{1-y_{i}}
$$
当y取1时，结果是左边的式子，取0是右边的式子，称为最大似然函数。但是由于每次取值都在0-1，连乘值太小，通常取对数log，上式就变为
$$
logL(\theta)=log\prod_{i=1}^{m}(e^{\theta x_{i}}/(1+e^{\theta x_{i}}))^{y_{i}}(1/(1+e^{\theta x_{i}}))^{1-y_{i}}\\
=\sum_{i=1}^{m}(y_{i}log\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}-(1-y_{i})log(1+e^{\theta x_{i}}))\\
=\sum_{i=1}^{m}(\theta x_{i}y_{i}-log(1+e^{\theta x_{i}}))
$$
也就是说，这个函数的意义是找到一个θ，使得输入x时输出y的概率最大

## 代价函数

LR的代价函数不是最小二乘法，而是采用交叉熵损失函数
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y_{i}log\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}-(1-y_{i})log(1+e^{\theta x_{i}}))
$$
显然，对数损失函数是把最大似然函数取负除以样本数量。对于最大似然函数，我们要使其最大；对于损失函数，我们要使其最小，两者是等价的，所以可以使用最大似然函数求解参数

对最大似然函数求偏导
$$
\frac{\partial logL(\theta)} {\partial \theta_{i}}=\sum_{i=1}^{m}(x_{i}y_{i}-\frac{e^{\theta x_{i}}} {1+e^{\theta x_{i}}}x_{i})=\sum_{i=1}^{m}(y_{i}-\hat h_{i}(x))x_{i}
$$
如果依然使其为0，是求解不出的，只能使用梯度下降等迭代方法得到最优解。这也是代价函数不使用最小二乘法的原因，该方法得到的代价函数不是凸函数，求得的解也只是局部最优解，不是全局最优解，而交叉熵函数就是凸函数

**凸函数**：函数上任意两点连线，线段上的每一个点都在函数内侧，特征是只有一个极值点

![](.\pictures\凸函数.png)

![](.\pictures\非凸函数.png)

## 梯度下降法

gradient descent，梯度是函数所有偏导数组成的向量，有方向，沿着梯度的方向是函数增长速度最快的方向。自然，梯度的反方向是函数下降速度最快的方向。我们要求代价函数的最小值，自然要求梯度，上文已得：

$$\bigtriangledown L(\theta)=\frac{\partial logL(\theta)} {\partial \theta_{i}}=\sum_{i=1}^{m}(x_{i}y_{i}-\frac{e^{\theta x_{i}}} {1+e^{\theta x_{i}}}x_{i})=\sum_{i=1}^{m}(y_{i}-\hat h_{i}(x))x_{i}$$

求解θ：

* 给θ初值，因为使函数最大（这里指最大似然函数）的θ值未知，要通过迭代求解，我们只能先“猜”一个
* 计算梯度
* 更新θ

$$
\theta_{j}=\theta_{j}+\alpha \bigtriangledown L(\theta)
$$

其中，α称为学习率，其实是步长。计算θ时应该同步更新，即用同一组参数计算。α取得过大，θ可以更快接近最优解，但在最优解附近会越过，左右来回“横跳”；取得过小，则迭代太慢，计算最优解时间长。所以应该在开始时给大的学习率，接近最优解时减小学习率。判断接近最优解：计算前后两次代价函数差值

同时，这里的学习率取正值，梯度也没有取负，采用加法，因为求的是最大似然函数，有最大值；若将梯度取反，就是代价函数，要求最小值，就要向梯度反方向“走”，应设为减法，这样θ更新的式子其实是一致的

* 检验：计算$$L(θ_{j+1})-L(\theta_{j})$$或者$$\theta_{j+1}-\theta_{j}$$，若该值小于设定的阈值，说明已经很接近最值了，可以把该参数认为是最优解；若没有小于阈值，从上面的第二步开始重复计算，直至满足。或者设定一个最大迭代次数

上面所述的梯度下降法是每次把所有样本的数据拿来计算，叫做**批量梯度下降（BGD）**，优点是遍历所有样本，结果精准；缺点是太耗时，计算慢，样本数量大时不适用

### 随机梯度下降

SGD，每次只随机抽取一个样本计算
$$
\theta_{j}=\theta_{j}+\alpha \bigtriangledown L(\theta)\\
\bigtriangledown L(\theta)=x_{j}y_{j}-\frac{e^{\theta x_{j}}} {1+e^{\theta x_{j}}}x_{j}=(y_{j}-\hat h_{j}(x))x_{j}
$$
这样做的好处是迭代速度快，但要迭代更多次，且计算得到的解未必是最优解

### 小批量梯度下降

MBGD，每次取部分的样本计算，综合了BGD和SGD的优点，计算速度快且准确
$$
\theta_{j}=\theta_{j}+\alpha \bigtriangledown L(\theta)\\
\bigtriangledown L(\theta)=\sum_{i=j}^{k}(x_{i}y_{i}-\frac{e^{\theta x_{i}}} {1+e^{\theta x_{i}}}x_{i})=\sum_{i=j}^{k}(y_{i}-\hat h_{i}(x))x_{i}
$$
求得参数θ后代入sigmoid函数就得到x对应的y值，表示在x这个特征取value(x)时，y有value(y)的可能性取正例1。按理说，sigmoid函数值越大，说明取1的概率越大，取0的概率越小，这正是我们期望的。但是解析一下sigmoid函数$$1/(1+e^{-\theta x})$$，可得，函数值越大，θ也越大，也就是某个特征x的权重大。这种情况是我们不希望见到的，因为一个特征的权重过大，那它影响也过大，其他特征的影响就会变小。比如房价，如果给面积过大的权重，那其他特征几乎不会有影响，房价将几乎全部由面积决定，显然不现实。实际上这是过多考虑了所有样本（包括噪声）的原因，过于注重拟合效果，想要把所有样本点都拟合在线上，这种情况称为**过拟合**，解决方法有：减少特征数量，正则化。但舍弃特征时有可能删除了有效的特征，多采用正则化

![](.\pictures\过拟合.png)

如果样本数量过少，或者特征过于简单，又会出现**欠拟合**的情况，模型不能很好描述数据集，拟合结果与实际结果偏差大。解决方法是增加特征维度

![](.\pictures\欠拟合.png)

## 正则化

前文已述，权重过大是用正则化解决，那么如何解决呢？抑制θ增大即可，通过在最大似然函数后减上正则项（惩罚项）/损失函数加上正则项

### 1范数

记为L1，θ服从零均值的拉普拉斯分布，表现为向量的各元素的绝对值之和。A=[a1,a2,a3]，L1(A)=|a1|+|a2|+|a3|，最大似然函数就变为
$$
L_{new}(\theta)=L(\theta)-\lambda||\theta||_{1}
$$
其中，$$||\theta||_{1}=||\theta_{2}||+||\theta_{3}||+...$$，λ称为超参数，用来调节θ。注意这里的θ不包括b(θ1)

显然，θ一味增大不能使得L(θ)最大，就起到了“惩罚”θ过大的作用。此外，L1可以使得大量θ变为0，从而让回归系数矩阵变成稀疏的，更方便计算，那些权重为0的特征也不会参与计算，会被过滤掉，起到了一个特征自动选择的作用。这样模型就更加简单，复合奥卡姆剃刀理论

*奥卡姆剃刀理论*：用最少的东西，做最好的事情

1范数是绝对值函数，在0处不可导，解决方法复杂，此处略过

### 2范数

记为L2，又叫岭回归，权值衰减，θ服从零均值的正态分布，表现为向量的各元素的平方和再开根号。A=[a1,a2,a3]，$$L2(A)=\sqrt{a1^{2}+a2^{2}+a3^{2}}$$，最大似然函数就变为
$$
L_{new}(\theta)=L(\theta)-\lambda||\theta||_{2}
$$
其中，$$||\theta||_{2}=\sqrt{\theta_{2}^{2}+\theta_{3}^{2}+...}$$

L2可以使得大量θ趋近0但不为0，特征不会减少，但权重会趋于0

对新的最大似然函数求梯度：
$$
\bigtriangledown L(\theta)=\sum_{i=1}^{m}(y_{i}-\hat h_{i}(x))x_{i}-2\sum_{i=1}^{m}\lambda\theta_{i}
$$

# 评价指标

## 混淆矩阵

confusion matrix，用来评价分类结果，作用：

* 计算分类的准确性，召回率
* 查看有多少分类被误分到哪一类下，方便区分设计

混淆矩阵有四个常用的指标：accuracy，precision，recall，miss，以下图为例说明：

首先说明下图中字母含义：

TP（true positive）：预测为正例，且预测正确即事实为正例，称为真阳性

TN（true negative）：预测为负例，且预测正确即事实为负例，称为真阴性

FP（false positive）：预测为正例，且预测错误即事实为负例，称为假阳性

FN（false negative）：预测为负例，且预测错误即事实为正例，称为假阴性

![](.\pictures\混淆矩阵.png)

* accuracy：所有预测正确的样本数/总样本数，(TP+TN)/(TP+TN+FP+FN)
* precision：预测正例中实际正例的比例，TP/(TP+FP)
* recall/真阳率：实际正例中预测正例的比例，TP/(TP+FN)
* miss：实际正例中预测负例的比例，FN/(TP+FN)
* 假阳率：实际负例中预测正例的比例，FP/(FP+TN)

可以看出，recall+miss=1，混淆矩阵对角线上的数字越大越好，说明分类准确

如果是多分类也是一样的，目标分类为正例，其他所有分类为负例

这里引用知乎的图片更形象地说明：

https://pic2.zhimg.com/v2-eb07516d12d4f465fca1dbf016a73c0f_r.jpg?source=1940ef5c

## ROC和AUC

以recall和precision为纵横坐标，可以得到PR曲线，两者不可兼得

![](.\pictures\PR曲线.png)

对应不同场景，两者取舍应该不同。像搜索，应该注重recall，尽可能多地把该分类召回；像垃圾邮箱监测，应该注重precision，分类尽可能准确

以recall和假阳率为纵横坐标，得到的就是ROC曲线。以二分类为例，每设置一个阈值，得到的分类结果必然不同，就会有不同的recall和假阳率，绘成ROC

![](.\pictures\ROC.png)

图上的实线的每一点都代表一个阈值，当阈值为0时，所有分类都是预测为正例，FN和TN都为0，则recall和假阳率都为1；阈值为1时，所有分类预测都为负例，TP和FP都为0，则recall和假阳率都为0。recall和假阳率是正相关，和阈值是负相关，但图中显然可得，recall增长速度更快

recall越大，实际正例中预测正例的部分越大；假阳率越大，实际负例中预测正例的部分越大。所以理想情况应该是，recall为1，假阳率为0，尽量靠近(0,1)点

### AUC

area under curve，就是ROC曲线下的面积，显然不大于1

* 当AUC=1时，称为完美预测，此时分类器必然存在至少一个阈值能使得所有分类正确；

* 当AUC=0.5时，称为随机预测，此时阈值在(0,0)到(1,1)的对角线上，预测效果完全随机；

* 当AUC介于0.5和1之间时，可以找到合适的阈值使分类尽量准确；

* 当AUC<0.5时，完全没有预测价值，可以取其预测的相反值

AUC到底是什么含义呢？以二分类为例，从正例样本中随机抽取一个，预测其为正例的概率设为p1（这个定义不就是真阳率吗？只不过只预测1个），从负例样本中抽取一个，预测其为正例的概率为p2（这个也是假阳率，只是预测1个），p1大于p2的概率就是AUC，所以AUC评价的是模型对样本的预测能力，值越大，把正例预测正确的概率就越大，把负例预测错误的概率就越小。这也是AUC优于上述precision，recall等指标的原因，后者在不同阈值下取值不同，准确度很受阈值影响；但AUC是比较两个概率值，不管阈值如何选取，真/假阳率的相对关系不会变，所以简明解释：当预测结果按概率升序排列时，AUC表示负样本排在正样本前面的概率

可以用下面的脚本命令求得：

```bash
cat auc.raw | sort -t$'\t' -k2g |awk -F'\t' '($1==0){++x;a+=y;}($1==1){++y;}END{print 1.0-a/(x*y);}'
```

```bash
cat auc.raw
#0       0.001  # 首列是标签，第二列是sigmoid函数值
#0       0.002
#0       0.003
#0       0.004
#1       0.005
#0       0.006
#1       0.007
#0       0.008 
#1       0.97
#1       0.99
```

解释参数含义：

* x：负样本个数
* y：正样本个数
* a：预测错误的样本数
* x*y：正负样本对

首先明确，第二列是sigmoid函数值，表征样本为正例的概率，又是升序排列，所以理想的情况应该是前面的第一列都是0，后面的第一列都是1。显然，我们的样本数据混入了两个“奸细”，我们要求的就是1减去这两个“奸细”排在前面的概率。所以遇上0就给x加一，遇上1就给y加一，当1后面还有0时，说明该1是“奸细”，给a加一，a/(x*y)就是预测错误的概率

当正负样本的分布发生变化时，ROC曲线的形状基本不变，PR曲线形状会发生剧烈变化 

## MSE、RMSE

Mean Squared Error，均方误差 
$$
\frac{1}{m}\sum_{i=1}^{m}(y_{i}-\hat h(x_{i}))^{2}
$$
Root Mean Squard Error ， 均方根误差 
$$
\sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_{i}-\hat h(x_{i}))^{2}}
$$




