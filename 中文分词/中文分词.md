数据挖掘的关键在于数据预处理，对于中文来说，中文分词是数据预处理的关键。中文分词的粒度由业务决定，比如搜索系统，结果要准确，分词粒度要细；推荐系统，尽量推荐出客户喜欢的，分词粒度要大

# 文本相似度

* 文本相似，但语义不相似：我吃饱饭了 VS 我吃不饱饭
* 语义相似，但字面不相似：刘亦菲真漂亮 VS 茜茜好好看

*不同垂类对文本相似度的定义不同*

## 解决方法

* 文本相似
  * LCS 最大公共子序列
  * 中文分词 + 余弦相似度

* 语义相似
  * 基于共现的行为（协同过滤）
  * 基于共现的窗口（词嵌入模型）

### 最大公共子序列

子序列是指一个序列经过删除若干元素后形成的新序列，删除的元素可以是不连续的，和子串不同。比如abcdefg的子序列可以是acf。最大公共子序列即Longest Common Subsequence，是指两个序列的公共子序列中最长的那个

#### LCS的求解方法

* 暴力法

设序列S1的长度为M，S2的长度为N，对于S1来说，存在2^M个子序列，S2存在2^N个子序列。遍历S1的每个子序列时都要和S2的子序列进行比较，判断是否是公共子序列并找出最大的，时间复杂度是O(2^M*2^N)

* 动态规划

设S1=(x1,x2,x3,....xm)，S2=(y1,y2,y3,....yn)，S1的前i个元素组成的序列是Xi，S2的前i个元素组成的序列是Yi

当xm=yn时，LCS的最后一位必然是xm/yn，此时就可以比较Xm-1和Yn-1；若xm!=yn，S1和S2的LCS必然是在Xm-1和Yn的LCS或者是Xm和Yn-1的LCS中，且取两者的最大值；由此可以一直往前推，直到任一序列长度为0
$$
LCS(X_{m},Y_{n})=\left\{\begin{matrix}
LCS(X_{m-1},Y_{n-1})+x_{m} & x_{m}= x_{n} \\ 
max(LCS(X_{m-1},Y_{n}),LCS(X_{m},Y_{n-1})) & x_{m}\neq x_{n} 
\end{matrix}\right.
$$


## ### 余弦相似度

将两句话分词，然后用向量表示，用两个向量的余弦判断，越接近1，向量的夹角越接近0，两句话就越相似，用来判断两个分好词的句子的文本相似度

$$cos\theta =(a\cdot b)/(|a||b|)$$

 ab分别代表两个向量

求余弦相似度的关键就在于将分词后的句子向量化：

* 用分词得到的结果形成一个词库，词库去重后形成一个新列A

* 在词库中分别统计两个句子的词频，按照A的顺序分别标注两句话对应词语的词频，**注意两句话的词语顺序应一致，且包含词库中所有词语**

例：用余弦相似度计算下面两句话的相似度

a：这场雨下得不小，我不想去上班了

b：这场雨太大了，我能不能不去上班

* 分词

这场/ 雨/ 下得 /不 /小/ 我/不/想/去/上班/了

这场/雨/太/大/了/我/能/不/能/不/去/上班

* 组成词库

这场/ 雨/ 下得 /不 /小/ 我/想/去/上班/了/太/大/能/

* 统计词频，向量化

这场/ 雨/ 下得 /不 /小/ 我/想/去/上班/了/太/大/能/

a：1/1/1/2/1/1/1/1/1/1/0/0/0

b：1/1/0/2/0/1/0/1/1/1/1/1/2

* 计算

$$\frac{a\cdot b}{|a||b|}=\frac{1+1+4+1+1+1+1}{\sqrt{13}\cdot \sqrt{16}}=0.693$$

# TF-IDF

用来找文章的关键词，需要事先分词

* TF ：词频/term frequency，计算方法是`某个词在当前文章中出现次数/当前文章总词数`或者`该词出现的次数/文章中最高频的词出现的次数`

* IDF ：反文档频率，文章最常出现的词一般是最重要的，但也不是绝对的，比如“是，的”这类词在所有文章都常见，不一定是本篇的关键词，叫做停用词（stop words），一般要剔除。IDF就是反映该词是否是停用词，计算方法是`log(文章总数/包含该词的文章数+1)`，公式中分母加一，是防止分母为0。值越小，越有可能是停用词

有些词很少见（IDF大），在某篇文章里却很常见（TF大），就可能是这篇文章的关键词

**TF-IDF的计算**

$$TFIDF=TF*IDF$$，与一个词在文章中出现的次数成正比，与包含该词的文章总数成反比，能够较为客观地显示文章的关键词，值越大，越重要

# 中文分词

## 分词常见方法

* 在哪个词前切分就在其处记1，否则记0

  如：中文/分词/有/几种/常见/的/切分/方法

​              1010110101100

* 从0开始，给每个字的前后空处标号，用分词处的标号表示

  如：中文/分词/有/几种/常见/的/切分/方法

  ​	    0中1文2分3词4有5几6种7常8见9的10切11分12方13法14

  ​	    {0，2，4，5，7，9，10，12，14}

* 字母表示位置，B表示begin，M表示middle，E表示end；对于单个字，S表示single，这也是jieba分词采用的方法

  如：中文/分词/有/几种/常见/的/切分/方法

  ​        BE/BE/S/BE/BE/S/BE/BE

* 从词典/语料库匹配：语料库的词也是积累来的。在语料库中匹配分词可以前向查找或者后向查找，一般是最大长度查找，贪心算法，尽可能多匹配，后向查找的准确率一般更高
  * 建立Trie树加速查找

    在root下建立从语料库查找的所有分词的分支

  ​       例：大学生活动中心，语料库的分词包括：大学生，学生，生活，活动，中心

  前向查找：从“大”开始，找到大学生；依次找到活动，中心

  ![](.\pictures\Trie树.png)

  后向查找：从“心”开始，找到中心，活动，找到“生学”后，因为是最大长度查找，接着把“大”找出来，查找到了大学生

   *后向查找是从文章最后开始匹配，所以分词也要倒过来*

  ![](.\pictures\Trie树2.png)

  * 词图切分

    有向无环图(DAG)，把待分词的句子分成单字，从0标号，把所有的可能罗列出来

  ​    例：广州本田雅阁汽车，语料库包括 广州，本田，广州本田，雅阁，本田雅阁，汽车

![](.\pictures\DAG.png)

对于 广：有三种情况，广，广州，广州本田，对应的词图为 0:[0,1,3]

​         州：有一种情况，州 ，对应词图 1:[1]

​         本：三种情况，本，本田，本田雅阁，对应 2:[2,3,5]

​         田：一种情况，田 ，对应 3:[3]

​         雅：两种情况，雅，雅阁 ，对应 4:[4,5]

​         阁：一种情况，阁 ，对应 5:[5]

​         汽：两种情况，汽，汽车， 对应  6:[6,7]

​         车：一种情况，车 ，对应 7:[7]

写成：{0:[0,1,3],1:[1],2:[2,3,5],3:[3],4:[4,5], 5:[5],6:[6,7],7:[7]}，这样就把所有分词的可能都罗列出来，再根据各个分词的概率判断如何分词

![](.\pictures\DAG2.png)

箭头上的数字表示该词组出现的概率，取了log对数

以后向查找为例，

车7：没有组合选择，概率是-9，选择[7:7]

汽6：两种组合，汽 /车，概率是-12-9；汽车，概率是-9

​           选择概率大的，[6:7]

阁5：一种组合，阁/汽车，概率是-10

​           选择[5:5,6:7]

雅4： 两种组合，雅/阁/汽车，概率是-11-10-9；雅阁/汽车，概率是-13-9

​           选择[4:5,6:7]

田3：只有一种，田/雅阁/汽车

本2：三种组合，本/田/雅阁/汽车，概率-7-9-13-9；本田/雅阁/汽车，概率-11-13-9；本田雅阁/汽 车，概率-15-9

​          选择[2:5,6:7]

州1：一种，州/本田雅阁/汽车

广0 ：三种，广/州/本田雅阁/汽车，概率-9-8-15-9； 广州/本田雅阁/汽车，概率-9-15-9；广州本田/ 雅阁/汽车，概率-17-13-9

​		   选择[0:1,2:5,6:7]

最终结果就是广州/本田雅阁/汽车

## 分词概率语言模型

对于一个句子C，C=C1C2……Cn，分词的结果是词串S，S=S1，S2，……Sm（m<=n），其中S1=C1C2，S2=C3....分词用概率表示就是P(S|C)，找到P(S|C)的最大值，意义就是对于C，切分方案S的可能性最大

### 概率模型推导

* P(S|C) ：对于句子C，被切分为S的概率

* P(C,S)：句子C和切分结果S同时存在的概率

* P(C)：句子C存在的概率，是个常数

* P(S)：切分结果S存在的概率

* P(C|S)：把切分结果S合并成句子C的概率，为1

则：

$$P(S|C)=\frac{P(C,S)}{P(C)}=\frac{P(S)P(C|S)}{P(C)}=\frac{P(S)}{P(C)}=\frac{P(S)}{const}\rightarrow P(S)$$

对于C，被切分为S的概率就是出现S的概率

比如：

对于句子C“南京市长江大桥”，存在两种切分方案：第一种S1，“南京市/长江/大桥”；第二种S2，“南京/市长/江大桥”

决定哪种分词方式需要比较P(S1|C) 和P(S2|C)，进一步化为比较P(S1)和P(S2)，因为S1和S2的分词都是独立的，直接将每个分词概率相乘
$$
P(S1)=P(“南京市/长江/大桥”)=P("南京市")P("长江")P("大桥")\\P(S2)=P(“南京/市长/江大桥”)=P("南京")P("市长")P("江大桥")
$$
显然P(S1)>P(S2)，故选择S1

但是，假如P(S1)和P(S2)都非常小，0.000000001和0.00000001比较，很可能会造成溢出，无法比较，故一般用$$log(P(S))$$把乘法化为加法，便于比较，$$log(0.0000000001)=-10,log(0.00000000001)=-11$$

### 语言模型

* 一元模型(unigram)：各分词相互独立

$$P(W_{1},W_{2},W_{3})=P(W_{1})P(W_{2})P(W_{3})$$

P(Wi)是Wi这个词在语料库出现的次数/语料库的总词数

事实上，分词一般都要考虑上下文的语境，不是独立的。比如“产品和服务”，可能会切成“产品，和服，务”，显然是不正确的

* 二元模型(Bigram)：一阶马尔科夫模型，每个状态依赖于前一个状态

$$P(W_{1},W_{2},W_{3})=P(W_{1})P(W_{2}|W_{1})P(W_{3}|W_{2})$$

* 三元模型(Trigram)：二阶马尔科夫模型，每个状态依赖于前两个状态

$$P(W_{1},W_{2},W_{3})=P(W_{1})P(W_{2}|W_{1})P(W_{3}|W_{1},W_{2})$$

* N元模型：N-1阶马尔科夫模型

$$P(W_{1},w_{2},...,W_{n})= P(W_{1})P(w_{2}|W_{1})P(w_{3}|W_{1},w_{2})…P(W_{n}|W_{1}w_{2}…W_{n-1})$$

称为概率的链规则

**马尔科夫模型（Markov Modle/MM）的参数**

以分词为例

![](.\pictures\MM.png)

* 状态概率：表示所有分词（状态）出现的概率

* 初始概率：$$\pi_{s_{1}}$$每个分词（状态）作为序列开始的概率，S1作为序列开始的次数/观测序列总数

* 状态转移概率：$$a_{s_{1},s_{2}}=p(w_{2}|w_{1})$$，在s1发生的情况下，S2发生的概率，由一个词切换到另一个词的概率。S2紧跟S1出现的次数/S1出现的总次数 

马尔科夫只能解决单序列问题，比如写文章，初始给一个“我”，MM可以根据状态转移概率不断输出新的字，形成一篇文章。但双序列问题无法解决，比如机器翻译，语音识别，需要输出一个全新的序列，初始概率和状态也是未知的。解决双序列问题，可以使用隐马尔科夫模型

#### 隐马尔科夫模型

又叫HMM，包括两个序列：

* 隐藏序列（状态序列）：未知，是一个马尔科夫序列，St+1只依赖于St（一阶马尔科夫）

* 观测序列：已知，各个序列值是独立的，数据通常由隐藏序列决定的，Ot只依赖于St

HMM参数：

* 状态序列：记为S，n阶马尔科夫模型，简化成一阶马尔科夫模型，即每个状态只和前一个状态有关

* 观测序列：记为O

* 状态转移概率：记为aij，指状态i变为j的概率

* 初始概率：Πi

* 发射概率：b(Oi|Si)，指第i个状态时该状态表现出观测值i的概率

![](.\pictures\HMM.png)

以分词为例，规定使用BMES来分词，同时需要标注其词性（jieba分词就是这么做的）

**状态序列**： (B,n)(E,n)(S,n)

**观测序列**： 广州/塔

其中，状态序列的n表示是名词

* 初始概率：初始状态是B，词性是名词的概率

* 状态转移概率：对于上一个状态是B且词性是名词，后面是E且词性是名词的概率

* 发射概率：状态为E，词性是名词且对应观测值为州的概率

HMM生成的过程就是：由初始概率乘状态转移概率得到下一状态，再乘上发射概率，得到对应状态的观测值，再继续乘状态转移概率。。。得到整个观测序列及其概率值

HMM满足的假设：

* 齐次马尔科夫假设：当前的状态只依赖于上一个状态，与其他状态无关，与所处的时刻也无关

* 观测独立假设：观测值只和对应的状态有关$$P(S_{1},O_{1},S_{2},O_{2})=P(S_{1},O_{1},S_{2})*P(O_{2}|S_{1},O_{1},S_{2})=P(S_{1},O_{1},S_{2})*P(O_{2}|S_{2})$$

**HMM解决的三个问题：**

* 估计问题

已知HMM的初始概率矩阵，状态转移概率矩阵，发射概率矩阵，状态序列，求出现某个观测序列的概率

解法：（1）暴力破解（遍历）

假设有n个状态，有m个观测值，有n^m个可能的观测序列，计算量太大

例：以经典的摸球为例，有一，二，三共计三个盒子，盒子一有5个红球，5个白球；盒子二有4个红球，6个白球；盒子三有7个红球，3个白球。第一次从盒子一，二，三摸球的概率分别是0.2，0.4，0.4，从盒子一摸球后接着从盒子一，二，三摸球的概率分别是0.5，0.2，0.3；从盒子二摸球后接着从盒子一，二，三摸球的概率是0.3，0.5，0.2；从盒子三摸球后接着从盒子一，二，三摸球的概率是0.2，0.3，0.5

问：假设最终摸球的结果是红白红，求该观测结果的可能性

解：首先我们把条件以更清楚的形式列出：

状态集合：{盒子一，盒子二，盒子三}

观测集合：{红球，白球}

观测序列：{红白红}

初始概率矩阵Π：

|  状态  | 概率 |
| :----: | :--: |
| 盒子一 | 0.2  |
| 盒子二 | 0.4  |
| 盒子三 | 0.4  |

状态转移概率矩阵aij

| 前一次状态\后一次状态 | 状态一（盒子一） | 状态二（盒子二） | 状态三（盒子三） |
| :-------------------: | :--------------: | :--------------: | :--------------: |
|   状态一（盒子一）    |       0.5        |       0.2        |       0.3        |
|   状态二（盒子二）    |       0.3        |       0.5        |       0.2        |
|   状态三（盒子三）    |       0.2        |       0.3        |       0.5        |

发射概率矩阵b(O|S)

| 状态/观测 | 红球 | 白球 |
| :-------: | :--: | :--: |
|  盒子一   | 0.5  | 0.5  |
|  盒子二   | 0.4  | 0.6  |
|  盒子三   | 0.7  | 0.3  |

b(O1|S1)指从状态一（盒子一）抽取红球的概率

三次都从盒子一摸取：

$$Π(1)*b(O1|S1)*a11*b(O2|S1)*a11*b(O1|S1)=0.2*0.5*0.5*0.5*0.5*0.5=0.0125$$

第一次从盒子一，后面两次从盒子二抽取：$$Π(1)*b(O1|S1)*a12*b(O2|S2)*a22*b(O1|S2)=0.2*0.5*0.2*0.6*0.5*0.4=0.0024$$

……

一共3^3=27个式子，把27个式子的结果相加就可得到最终观测结果为{红白红}的概率

27个式子的计算量对于计算机也许不大，但如果观测序列长度是100呢？那就是3^100，计算太麻烦了

（2）前向算法

对于第一次观测结果来说，存在n个状态，对于第二次观测结果，也存在n个状态，就是`n*n`个状态组合，把`n*n`个状态组合按照第二个状态分为n组，把n个第一次观测的状态概率求和（要求n个是同一个第二次的状态）。以第二次观测结果为起点，对第三次观测结果进行同样的计算，仍需要`n*n`次计算，直到求到最后一一个观测结果的n种状态概率，把n个概率相加，得到最终观测结果的概率，一共要进行`n*n`的计算m次，所以复杂度是`m*n^2`，显然当m比较大时，比暴力求解的n^m的计算复杂度要小得多

解上例：

1. 第一次观测为红：

   从盒子一抽取：$$c11(红)=Π(1)*b(o1|s1)=0.2*0.5=0.1$$

   从盒子二抽取：$$c12(红)=Π(2)*b(o1|s2)=0.4*0.4=0.16$$

   从盒子三抽取：$$c13(红)=Π(3)*b(01|s3)=0.4*0.7=0.28$$

 c11(红)：第一个1表示时刻1，第二个1表示状态1，红表示观测结果  

2. 第一次观测为红，第二次观测为白：

​      从盒子一抽取：$$c21(白)=(c11(红)*a11+c12(红)*a21+c13(红)*a31)*b(o2|s1)=(0.1*0.5+0.16*0.3+0.28*0.2)*0.5=0.077$$

​      从盒子二抽取：$$c22(白)=(c11(红)*a12+c12(红)*a22+c13(红)*a32)*b(o2|s2)=(0.1*0.2+0.16*0.5+0.28*0.3)*0.6=0.1104$$

​      从盒子三抽取：$$c23(白)=(c11(红)*a13+c12(红)*a23+c13(红)*a33)*b(o2|s3)=(0.1*0.3+0.16*0.2+0.28*0.5)*0.3=0.0606$$

3. 第一次观测红，第二次观测白，第三次观测红

​      从盒子一抽取：$$c31(红)=(c21(白)*a11+c22(白)*a21+c23(白)*a31)*b(o1|s1)=(0.077*0.5+0.1104*0.3+0.0606*0.2)*0.5=0.04187$$

​     从盒子二抽取：$$c32(红)=(c21(白)*a12+c22(白)*a22+c23(白)*a32)*b(o1|s2)=(0.077*0.2+0.1104*0.5+0.0606*0.3)*0.4=0.035512$$

​     从盒子三抽取：$$c33(红)=(c21(白)*a13+c22(白)*a23+c23(白)*a33)*b(o1|s3)=(0.077*0.3+0.1104*0.2+0.0606*0.5)*0.7=0.052836$$

​    所以，观测结果是{红白红}的概率是：$$c31(红)+c32(红)+c33(红)=0.04187+0.035512+0.052836=0.130218$$

![](.\pictures\前向算法.png)

比较暴力求解和前向算法：

对于m个观测的n个可能的状态，暴力法每一次都从第一个观测的第一个可能状态开始尝试，比如上例，计算完从第一个盒子-第一个盒子-第一个盒子，还会计算从第一个盒子-第一个盒子-第二个盒子抽取后，第一个/第二个盒子-第一个盒子-第二个盒子/第三个盒子抽取。。。其实从第一个/第二个-第一个盒子已经计算过了，会有重复计算。前向算法就是每次只考虑两个观测结果，将上一个观测结果的所有可能状态都累加，就只需要计算本次的状态和下一次的，如本例，第一个-第一个盒子，第二个-第一个盒子，第三个-第一个盒子，将三个状态相加，那计算第一个-第一个-第一个，第二个-第一个-第一个，第三个-第一个-第一个，就只需要计算（第一个，第二个，第三个）-第一个-第一个就可以了，不需要重复计算

（3）后向算法

逻辑和前向算法一样，只不过是从最后的观测结果开始，往前计算。值得注意的是，前向算法对于第一个状态，存在初始概率，后向算法对于最后一个状态，也存在一个后向概率，β=1。可以认为对于最后一个观测结果，不管状态是什么，观测结果都是确定的

1. 第三次观测为红：

​     从盒子一抽取：$$β31(红)=1$$

​     从盒子二抽取：$$β32(红)=1$$

​     从盒子三抽取：$$β33(红)=1$$

2. 第二次观测为白，第三次观测为红：

​     从盒子一抽取：$$β21(白)=β31(红)*a11*b(o1|s1)+β32(红)*a12*b(o1|s2)+β33(红)*a13*b(o1|s3)=1*0.5*0.5+1*0.2*0.4+1*0.3*0.7=0.54$$

​    从盒子二抽取：$$β22(白)=β31(红)*a21*b(o1|s1)+β32(红)*a22*b(o1|s2)+β33(红)*a23*b(o1|s3)=1*0.3*0.5+1*0.5*0.4+1*0.2*0.7=0.49$$

​    从盒子三抽取：$$β23(白)=β31(红)*a31*b(o1|s1)+β32(红)*a32*b(o1|s2)+β33(红)*a33*b(o1|s3)=1*0.2*0.5+1*0.3*0.4+1*0.5*0.7=0.57$$

3. 第一次观测为红，第二次观测为白，第三次观测为红：

​     从盒子一抽取：$$β11(红)=β21(白)*a11*b(o2|s1)+β22(白)*a12*b(o2|s2)+β23(白)*a13*b(o2|s3)=0.54*0.5*0.5+0.49*0.2*0.6+0.57*0.3*0.3=0.2451$$

​    从盒子二抽取：$$β12(红)=β21(白)*a21*b(o2|s1)+β22(白)*a22*b(o2|s2)+β23(白)*a23*b(o2|s3)=0.54*0.3*0.5+0.49*0.5*0.6+0.57*0.2*0.3=0.2622$$

​    从盒子三抽取：$$β13(红)=β21(白)*a31*b(o2|s1)+β22(白)*a32*b(o2|s2)+β23(白)*a33*b(o2|s3)=0.54*0.2*0.5+0.49*0.3*0.6+0.57*0.5*0.3=0.2277$$

​    考虑初始情况：  $$β(红白红)=β11(红)*Π(1)*b(o1|s1)+β12(红)*Π(2)*b(o1|s2)+β13(红)*Π(3)*b(o1|s3)=0.2451*0.2*0.5+0.2622*0.4*0.4+0.2277*0.4*0.7=0.130218$$

   结果与前向算法相同      

* 解码问题

已知观测序列，HMM的初始概率矩阵，状态转移概率矩阵，发射概率矩阵，求状态序列的最大可能

同样是上述的例子，求观测序列是“红白红”时的最大概率的状态序列

解决这个问题的方法同样有两种，暴力求解法和维特比（viterbi）算法

（1）暴力求解法

和上一个问题的暴力求解法一样，只不过对于27个式子，不用求和，比较出最大概率的那个状态序列即可

（2）viterbi算法

和前向/后向算法的思想一致，也是以两个观测的状态矩阵为一组，以第二个观测的状态分组，求出两次状态概率乘积最大的那一组，再以第二个状态为起点，一直计算到最后。不同的是，viterbi算法是求最大概率的状态序列的，不是求对应观测序列的概率，故每一步不需要把各状态序列的概率相加，只需要找到最大的那一组即可。在求解每个时刻的最大概率状态时，要对其进行标记，以便于在求解完整个状态序列时可以回溯，得到最大概率的状态序列

例：同上

解：

1. 第一次观测为红：

​       从盒子一抽取：$$c11(红)=Π(1)*b(o1|s1)=0.2*0.5=0.1$$

​       从盒子二抽取：$$c12(红)=Π(2)*b(o1|s2)=0.4*0.4=0.16$$

​       从盒子三抽取：$$c13(红)=Π(3)*b(01|s3)=0.4*0.7=0.28$$

 第一次观测状态三概率最大  

2. 第一次观测为红，第二次观测为白：

​     从盒子一-盒子一抽取：
$$
c21\_1(白)=c11(红)*a11*b(o2|s1)=0.1*0.5*0.5=0.025
$$

​     c21_1指第二次观测状态为一，上一个状态为一，即两次分别从盒子一-盒子一抽取

​    从盒子二-盒子一抽取：
$$
c21\_2(白)=c12(红)*a21*b(o2|s1)=0.16*0.3*0.5=0.024
$$
​    从盒子三-盒子一抽取：

​										$$c21\_3(白)=c13(红)*a31*b(o2|s1)=0.28*0.2*0.5=0.028$$

对于观测二的状态一，状态三-状态一概率最大，为0.028

   

   从盒子一-盒子二抽取：
$$
c22\_1(白)=c11(红)*a12*b(o2|s2)=0.1*0.2*0.6=0.012
$$
  从盒子二-盒子二抽取：
$$
c22\_2(白)=c12(红)*a22*b(o2|s2)=0.16*0.5*0.6=0.048
$$
   从盒子三-盒子二抽取：

​								    	$$c22\_3(白)=c13(红)*a32*b(o2|s2)=0.28*0.3*0.6=0.0504$$

对于观测二的状态二，状态三-状态二概率最大，为0.0504

   

  从盒子一-盒子三抽取：
$$
c23\_1(白)=c11(红)*a13*b(o2|s3)=0.1*0.3*0.3=0.009
$$
  从盒子二-盒子三抽取：
$$
c23\_2(白)=c12(红)*a23*b(o2|s3)=0.16*0.2*0.3=0.0096
$$
  从盒子三-盒子三抽取：

​									   $$c23\_3(白)=c13(红)*a33*b(o2|s3)=0.28*0.5*0.3=0.042$$

对于观测二的状态三，状态三-状态三概率最大，为0.042



3. 第一次观测红，第二次观测白，第三次观测红

对于第三个观测结果，状态一有9种选择，盒子一-盒子一-盒子一，盒子一-盒子二-盒子一，。。。盒子三-盒子三-盒子一。但上一步已经分别算出从观测一到观测二的三种状态的最大概率，故只需挑出这三种情况和观测三的状态一计算即可。这里容易犯的错误是直接选取三个概率的最大值和观测三的状态一做计算，因为状态间还存在转移概率，不能仅凭状态概率武断判断哪种情况最优

   从盒子三-盒子一-盒子一抽取：
$$
c31\_1(红)=c21\_3(白)*a11*b(o1|s1)=0.028*0.5*0.5=0.007
$$
   从盒子三-盒子二-盒子一抽取：
$$
c31\_2(红)=c22\_3(白)*a21*b(o1|s1)=0.0504*0.3*0.5=0.00756
$$
   从盒子二-盒子三-盒子一抽取：
$$
c31\_3(红)=c23\_2(白)*a31*b(o1|s1)=0.0096*0.2*0.5=0.00096
$$
对于观测三的状态一，状态三-状态二-状态一概率最大，为0.00756



   从盒子三-盒子一-盒子二抽取：
$$
c32\_1(红)=c21\_3(白)*a12*b(o1|s2)=0.028*0.2*0.4=0.00224
$$
   从盒子三-盒子二-盒子二抽取：
$$
c32\_2(红)=c22\_3(白)*a22*b(o1|s2)=0.0504*0.5*0.4=0.01008
$$
   从盒子二-盒子三-盒子二抽取：
$$
c32\_3(红)=c23\_2(白)*a32*b(o1|s2)=0.0096*0.2*0.4=0.000768
$$
对于观测三的状态二，状态三-状态二-状态二概率最大，为0.01008



   从盒子三-盒子一-盒子三抽取：
$$
c33\_1(红)=c21\_3(白)*a13*b(o1|s3)=0.028*0.3*0.7=0.00588
$$
   从盒子三-盒子二-盒子三抽取：
$$
c33\_2(红)=c22\_3(白)*a23*b(o1|s3)=0.0504*0.2*0.7=0.007056
$$
   从盒子二-盒子三-盒子三抽取：
$$
c33\_3(红)=c23\_2(白)*a33*b(o1|s3)=0.0096*0.5*0.7=0.00336
$$
对于观测三的状态三，状态三-状态二-状态三概率最大，为0.007056

 故，状态三-状态二-状态二概率最大，状态序列为{盒子三，盒子二，盒子二}

![](.\pictures\维特比算法.png)

这里的viterbi算法采用的前向计算，也可用后向计算，结果是一样的

* 学习问题

只知道观测序列，要求建立一个HMM，实现观测结果

采用EM算法，作者暂时还没搞懂，QAQ

# jieba分词

存在三种模式：

* 精准模式：默认方式，适合文本分析

* 全模式：把所有词语都罗列，会出现歧义

* 搜索引擎模式：在精准模式的基础上对长词再次划分，粒度更细

jieba分词过程：

* 将句子切成单个字，加载语料库。*语料库有三列：第一列，词语；第二列，词频TF；第三列，词性*
* 采用Trie树实现词图的高速扫描，形成词库，用DAG图划分分词（仅限于登陆词）的所有可能

* 采用动态规划查找最优路径，找出基于词频的最大切分组合

* 对于未在语料库中出现的词语，采用隐马尔科夫模型及viterbi算法找出其最有可能的状态，划分分词。在jieba分词里，观测序列对应分好词的句子，状态序列是B,M,E,S

*jieba分词，将分词和词性标注都完成，贪心算法，最大概率组合长的词，粒度粗，分词之前进行数据清洗，去除标点符号*

```python
jieba.cut() # 分词
jieba.posseg.cut() # 分词加词性标注
jieba.analyse.extract_tags() # 提取关键词，并返回权重
```













