决策树（decision tree）可以解决分类和回归问题

决策树生成：选定某个特征作为根结点分类，继续选择其他特征分类，直至标签值唯一/走到了叶子节点，就得到了最终的分类结果。每个内部节点代表一个特征，叶子节点代表标签的分类/回归值，每一步都是if-then的判断

比如下表表示有多个特征的记录，采用决策树方法生成的树如下图

| 计数 | 年龄 | 收入 | 学生 | 信誉 | 是否购买 |
| :--: | :--: | :--: | :--: | :--: | :------: |
|  64  |  青  |  高  |  否  |  良  |    否    |
|  64  |  青  |  低  |  否  |  优  |    是    |
| 128  |  中  |  高  |  否  |  良  |    是    |
|  64  |  老  |  中  |  否  |  良  |    是    |
|  64  |  青  |  中  |  是  |  良  |    是    |
|  64  |  青  |  中  |  否  |  优  |    否    |
|  64  |  老  |  低  |  是  |  优  |    否    |

![](.\pictures\决策树1.png)

显然，选择不同的特征做为根节点开始，内部节点选择顺序不同，生成的决策树也不一样。根据选择根节点-内部节点的策略的不同，可以分为ID3、C45、cart算法，依据是信息熵entropy/香农熵

**香农熵：**用H表示，描述信息分类结果的混乱程度、不纯度、不确定性，结果越小越确定。如一个袋子里有N个球，任意拿出一个来判断是否是红色。若N个球都是红色，熵就为0，结果确定；若一半红一半蓝，熵为1，结果最不确定
$$
H=-∑(p(i)log(p(i)))
$$
`p(i)表示第i个标签的概率`

## ID3

信息增益：G=H-E，其中，G表示每个特征的信息增益，H表示标签的信息熵，E表示每个特征的平均信息期望
$$
E=∑\frac {s_{1}+s_{2}+...s_{i}} {S}H_{i}
$$
Hi是对应特征的第i个属性的信息熵，(s1+s2...+si)/S表示对应特征的第i个属性的所有样本个数占该特征样本总个数的比值

ID3就是优先选择信息增益大的特征，步骤：根据标签求出信息熵H，对每一个特征分类，计算各类属性的分布和信息熵，将分布概率值和信息熵相乘求和，得到每个特征的平均信息期望E，用信息熵H-E，得到每个特征的信息增益，按最大的特征划分，重复，直至标签唯一（结果纯净，信息熵为0）

比如上例：1.首先计算标签的信息熵
$$
p(买)=320/(320+192)=0.625\\
p(不买)=192/(320+192)=0.375 \\

H=-p(买)log(p(买))-p(不买)log(p(不买))\\=-0.625*log0.625-0.375*log0.375=0.9544
$$
2.计算按年龄特征分类的各样本集的信息熵
$$
p(青年)=256/512=0.5 \\
p(买|青年)=128/256=0.5\\
p(不买|青年)=128/256=0.5 \\
H(青年)=-p(买|青年)log(p(买|青年))-p(不买|青年)log(p(不买|青年) \\
=-0.5*log0.5-0.5*log0.5=1 \\

p(中年)=128/512=0.25 \\
p(买|中年)=128/128=1\\
p(不买|中年)=0 \\
H(中年)=-p(买|中年)log(p(买|中年))-p(不买|中年)log(p(不买|中年) \\
=0 \\

p(老年)=128/512=0.25 \\
p(买|老年)=64/128=0.5\\
p(不买|老年)=64/128=0.5 \\
H(老年)=-p(买|老年)log(p(买|老年))-p(不买|老年)log(p(不买|老年) \\
=-0.5*log0.5-0.5*log0.5=1 \\
$$
3.计算年龄特征的平均信息期望
$$
E(年龄)=p(青年)H(青年)+p(中年)H(中年)+p(老年)H(老年) \\
      =0.5*1+0.25*0+0.25*1=0.75
$$
4.求信息期望
$$
G(年龄)=H-E(年龄)=0.9544-0.75=0.2044
$$
同理，求得按收入，学生，信誉特征的信息期望，用最大的信息期望的特征作为根节点，并依次选取下一个子节点

ID3算法结束的情况：

* 按特征划分下去，标签唯一，即信息熵为0
* 特征划分完毕，标签仍然不唯一，只能采取少数服从多数，归类为最多个数的那一类

缺点：

* 按信息增益最大的特征来划分，倾向于挑选属性多的特征（贪婪性），但并不是属性越多的特征越好，比如user_id

* 不适用于连续变量

## C4.5

为了解决ID3的弊端，使用C4.5算法。在对特征按属性划分时，也求该特征的属性的熵。用信息增益/该熵作为信息增益率，根节点-内部节点的选择标准由信息增益变为信息增益率
$$
H_{年龄}=-p(青年)log(青年)-p(中年)log(中年)-p(老年)log(老年) \\
=-0.5*log(0.5)-0.25*log(0.25)-0.25*log(0.25)=3/2 \\

G_{i}(年龄)=G(年龄)/H_{年龄}=0.2044/(3/2)=0.1363
$$
分别求得所有特征的信息增益率，C4.5倾向于先选择信息增益大于平均值的，再从中选择信息增益率大的

## CART

ID3和C4.5只能解决分类问题，CART可以解决分类和回归问题

不同于ID3和C4.5的决策树可以有多个分支，CART算法的决策树叫二叉回归树，只能有两个分支，选择子节点的依据也不是信息增益/信息增益率，而是Gini系数，选择Gini系数小的

#### 分类

Gini系数计算方法：

![](.\pictures\cart1.png)

#### 回归

连续型的变量，无法用二叉树表示，可以人为地划分区间，通过取中间值的方式用大于和小于该值划分

![](.\pictures\cart2.png)

连续型变量计算：

1.对所有相邻变量取中值，从第一个中值开始划分为两个区间，分别计算每个区间的平均值

2.分别计算每个区间和平均值的误差的平方和，取最小的那个划分区间

3.按照划分的区间求每个值和平均值的误差，得到新变量组，重复第一步

4.一直迭代到n层

例：

![](.\pictures\cart3.png)

切分点： 1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5

切分点取1.5时，划分的两个区间：R1={1}，R2={2,3,4,5,6,7,8,9,10}

R1平均值：5.56，R2平均值7.5

R1的误差平方和：m1=0  R2的误差平方和：m2=15.72 总误差m=0+15.72=15.72

切分点取2.5时，求其误差和 。。。

得到所有切分点的总误差m(s)

![](.\pictures\cart4.png)

m(s)在切分点为6.5时最小，就把区间切分为{1，2，3，4，5，6}和{7，8，9，10}

计算两个区间的平均值，得到：

![](.\pictures\cart5.png)

将原变量值按6.5划分为两个区间，分别减去两个平均值，得到新变量组

![](.\pictures\cart6.png)

再按1.5，2.5.。。。9.5划分，重复上述计算

![](.\pictures\cart7.png)

直至第n次

![](.\pictures\cart8.jpg)

![](.\pictures\cart9.jpg)

#### 剪枝

在决策树进行节点划分时进行判断，选择进不进入该节点以简化决策过程

* 前向剪枝 ：也叫预剪枝，在决策树生成过程中，进行节点划分前进行估计，若信息增益大于阈值就划分，否则不划分

优点：降低了过拟合的风险，减少了训练的时间和测试时间的开销

缺点：预剪枝有可能会使很多分支没有展开，有一些可能在当前分支不能带来效应，但是在这个分支基础上进行后续划分会带来一定提升。所以可能会带来欠拟合

* 后向剪枝：决策树生成后，从叶子节点开始对同一父节点的子节点进行合并，若合并之后熵增小于阈值就合并，否则保留原子节点

优点：在防止过拟合的同时也降低了欠拟合的可能性

缺点：后剪枝是在完成整个决策树以后开始的，并且从底层开始慢慢往上剪，所以花费的时间会比预剪枝大

后向剪枝可分为：基于错误率（REP）、悲观剪枝（PEP）、代价复杂度（CCP）

