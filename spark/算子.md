# SparkConf

 Spark应用程序的配置，将各种Spark参数设置为键值对格式。 SparkConf将从`spark.*` 属性中加载值，在这个对象上设置的任何参数都将优先于系统属性

*一旦将SparkConf对象传递给Spark，它就会被克隆，无法再修改*

常用方法：

* contains(key)：返回True/False，判断key是否在配置里
* get(key, defaultValue = None)：得到key的配置值，没有返回默认值
* getAll()：获取所有key的配置信息，以列表形式返回，列表内容是key-value对
* set(key,value)：设置key的配置值为value
*  setAll(pairs)：设置多个key的配置值，paris是列表形式，列表内容key-value对
* setAppName(value)：设置application的名字
* setMaster(value)：设置连接的master的URL
* setExecutorEnv(key=None,value=None,pairs=None)：设置executor的环境变量
* setIfMissing(key,value)：若key不存在，设置value
* setSparkHome(value)：设置从节点上安装spark的路径

SparkConf支持链式方法，如

```python
conf.setMaster("local").setAppName("My app")
```

# SparkContext

Spark的入口点，是连接spark集群的地方，只有一个，想要创建第二个必须先停止第一个

## 共享变量

spark启动之后，会产生一个driver，从节点上会产生executor，只有算子操作是进行在executor上的，其他操作都在driver上。这就导致一个问题：算子中的变量和之前定义的变量不互通，因为一个在driver上，一个在executor上。为了解决这个问题，spark允许driver将变量分发给executor。但是每个变量可能会被重复发送到同一个executor多次，造成内存溢出。解决方法就是共享变量，有两种：broadcast和accumulator，分别解决不同的问题

### broadcast

通过`BroadCast=sc.broadcast(value)`定义一个广播变量，通过`BroadCast.value`获取该变量值。使用广播变量的优势在于，会将变量封装作为只读变量传递给executor，每个只传递一次。注意，广播变量只能在driver端定义，不能在executor定义

* destroy()：销毁与此rdd有关的数据和元数据
* dump(value,file)：把value写入file
* load(file)：读取file内容
* unpersist()：删除广播的副本，之后若再使用要重新发送
* value：获得广播值

```python
sc = SparkContext('local', 'test')
# driver定义
b = sc.broadcast([1, 2, 3, 4, 5])
b.value
#[1, 2, 3, 4, 5]
# executor使用
sc.parallelize([0, 0]).flatMap(lambda x: b.value).collect()
#[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]
b.unpersist()
```

虽然广播解决了可能内存溢出的问题，但是，在executor上只能访问广播变量，不能修改（driver可以修改），想要解决这个问题就要用到累加器accumulator

### accumulator

通过`Accumulator=sc.accumulator(init_value)`定义一个累加器变量并赋予初值，通过`Accumulator.value`获取累加器变量的值。累加器变量在executor修改，在driver上定义初值但只能访问。之所以叫累加器，是因为accumulator只能增加，不能减少

* add(term)：加上term
* value：获得accumulator值

```python
# 在driver中定义
accum = sc.accumulator(0)

# 在executor中累加
sc.parallelize(1 to 10).foreach(accum += 1)

# 在driver中输出
accum.value
# 10
```

## 文件

文件读取之后，生成的是RDD

## pickleFile

根据task的数目生成文件数的序列化文件，rdd会保存为sequenceFile，默认批处理文件数为10

```python
# 保存
rdd.saveAsPickleFile(path,batchsize=10)
# 读取
sc.pickleFile(path)
```

### textFile

文本文件，编码必须是utf-8，将文件的每行作为RDD的一个值，可压缩

```python
# 保存
rdd.saveAsTextFile(path)
# 读取
sc.textFile(path)
```

### wholeTextFiles

读取一个文本文件目录作为RDD，每个元素是一个键值对，键是文件名，值是一个textFile。因为文件都是加载到内存中的，所以更适用于小文件

```python
# 读取
sc.wholeTextFiles(path)
# 保存
rdd.saveAsTextFiles(path)
```

### sequenceFile

是一种(key,value)的二进制格式的Hadoop文件，可以压缩

```python
# 读取
sc.sequenceFile(path)
# 保存
rdd.saveAsTextFile(path)
```

属性：

* applicationId：spark作业的标识，表示spark的运行模式。local-表示本地模式，application_表示yarn集群模式
* startTime：返回启动sparkcontext的时间
* version：查看spark版本

方法：

* stop()：停止一个sparkcontext
* addFile(path,recursive=False)：给参与spark作业的其他节点添加文件，可以是本地文件，hdfs文件，HTTP等。recursive设为True，可以添加目录（但只适用于Hadoop文件系统）
* addPyFile(path)：给任务添加py依赖文件，可以是本地，hdfs，HTTP等
* binaryFiles(path)：添加二进制文件，返回值是键值对，键为路径，值为文件内容
* cancelAllJobs()：取消所有正在运行/计划运行的作业
* setJobGroup(groupId,description)：给所有正在运行的作业分配一个组id
* cancelJobGroup(groupId)：取消指定组的作业
* emptyRDD()：创建一个空的RDD，没有元素或分区
* parallelize(data,slice)：创建一个python本地的data的RDD，可以并行化，被分成slice个partition
* range(start,end,step)：和python的range一样，生成从start开始到end结束（不包括在内）步长为step的数组
* runJob(RDD,func,partitons=None)：在指定的RDD执行func函数，可以指定在哪些partition上运行
* show_profiles()：输出配置文件的统计信息

# SparkFiles

主要是针对`SparkContext.addFile()`，这个类只有类方法，不能创建实例

* get(filename)：读取`addFile`上传文件的绝对路径
* getRootDirectory()：获得添加文件的根目录

# RDD

属性：

* context：查看创建此RDD的sparkcontext是哪个

方法：

* aggregate(init_value,seq,comb)：对RDD和init_value先进行seq函数操作，再进行comb函数的操作。其中，seq是对RDD所有分区单独并行操作，comb是对分区后的结果操作

```python
seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
'''
RDD分为两个partition：[1,2]和[3,4],init为(0,0)
seqOp函数：
x代表init，y代表RDD每个分区
第一个partition：
0+1，0+1
0+1+2，0+1+1
结果：(3,2)
第二个partition：
0+3，0+1
0+3+4，0+1+1
结果：(7,2)
combOp函数：
x对应第一个分区，y对应第二个分区
3+7，2+2
结果：(10, 4)
'''
sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
#(0, 0)
```

* aggregateByKey(value,seqOp,combOp)：和aggregate大致一样，但是每个分区和分区之间都是按key操作

```python
seqOp = (lambda x, y: x if x > y else y)
combOp = (lambda x, y: x + y)
sc.parallelize([(1,1),(1,2),(2,1),(2,3),(2,4),(1,7)],2).aggregateByKey(3,seqOp,combOp)
'''
seqOp：
(1,1),(1,2),(2,1)=>(1,(1,2,3)),(2,(1,3))=>(1,3),(2,3)
(2,3),(2,4),(1,7)=>(2,(3,4)),(1,7)=>(2,4),(1,7)
combOp：
(1,3+7),(2,3+4)=>(1,10),(2,7)
'''
```

* cache()：持久化操作，使用默认的存储级别保留RDD，但存储级别只能是内存
* persist()：持久化操作，存储级别可以是内存和磁盘，但若未指定，默认使用内存
* unpersist()：解除持久化操作，从硬盘和内存删除
* cartesian(rdd2)：返回rdd和rdd2的笛卡尔积

```python
rdd1 = sc.parallelize([1,2])
rdd2 = sc.parallelize([3,4])
R = rdd1.cartesian(rdd2).collect
print(R)
# [(1,3),(1,4),(2,3),(2,4)]
```

* coalesce(partition_num,shuffle=False)：对RDD重新分区为partition_num个，shuffle默认为False，即窄依赖。也就是说，若rdd原分区小于待重新划分的分区数，则shuffle必须设为True

```python
sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).collect()
# [[1, 2, 3, 4, 5]]
```

* repartition(partition_num,shuffle=True)：和coalesce用法一致，但shuffle默认True
* cogroup(rdd2)：将rdd1和rdd2按key聚合，但不同rdd会分开表示

```python
x = sc.parallelize([("a", 1), ("b", 2),("b", 3)])
y = sc.parallelize([("a", 2)])
[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]
#[('a', ([1], [2])), ('b', ([2,3], []))]
```

* groupWith(rdd1,*rdd2)：用法同cogroup，但支持多个RDD

```python
w = sc.parallelize([("a", 5), ("b", 6)])
x = sc.parallelize([("a", 1), ("b", 4)])
y = sc.parallelize([("a", 2)])
z = sc.parallelize([("b", 42)])
[(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]
#[('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]
```

* reduceByKey
* collect()：返回RDD所有元素的列表
* collectAsMap()：返回RDD元素的字典形式

```python
m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()
# {1:2,3:4}
```

* combineByKey(createCombiner,mergeValue,mergeCombiners,partition_num)：类似于MapReduce的combiner预聚合，对于不同partition的数据，先在map端聚合，再在reduce端聚合。提供三个接口可以自定义函数，在执行自定义函数之前，这个算子就已经在各个partition将同一key的数据分组完成了
  * createCombiner：创建combiner这个容器，只对同一分区的各key的第一个元素作用
  * mergeValue：在各分区完成combine，对同一分区的同一key的所有其他元素作用
  * mergeCombiners：在所有分区完成combine，对所有分区的同一key的所有元素作用

```python
x = sc.parallelize([("a", 1), ("b", 1), ("a", 2),("a", 3)])
def to_list(a):
    return [a]

def append(a, b):
    a.append(b)
    return a

def extend(a, b):
    a.extend(b)
    return a

sorted(x.combineByKey(to_list, append, extend).collect())
'''
1.启动该算子之后，rdd：
partition1:('a',(1,2))
partition2:('b',1),('a',3)
2.to_list:
partition1:('a',([1],2))
partition2:('b',[1]),('a',[3])
3.append:
partition1:('a',([1,2]))
partition2:('b',[1]),('a',[3])
4.extend:
[('a', [1,2,3]), ('b', [1])]
'''
# 用combineByKey实现reduceByKey
reduceByKeyRDD = rdd.combineByKey(lambda a:a,lambda a,b:a+b,lambda a,b:a+b)
```

* count()：返回RDD的元素个数
* countByKey()：返回RDD每个key和对应元素数量，形式是字典
* countByValue()：返回RDD每个元素值的数量，形式是字典
* distinct(partion_num)：返回去重后的RDD
* filter(func)：按func的条件过滤RDD
* first()：返回RDD的第一个元素
* map(func)：映射函数，对RDD的所有元素执行func
* keyBy(func)：用法等同于(map(func(x)),x)，返回这样一个元组对，相当于给原来的元素加了key

```python
x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)
y = sc.parallelize(zip(range(0,5), range(0,5)))
[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]
#[(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]
```

* mapPartitions(func)：对RDD的partition执行func

```python
sc.parallelize([1,2,3,4],2).mapPartitions(lambda x,y:x+y).collect()
# [3,7]
```

* mapPartitionsWithIndex(func)：和mapPartitions一样，多了一个参数是partition的索引，从0开始

```python
rdd = sc.parallelize([1, 2, 3, 4], 4)
def f(splitIndex, iterator): yield splitIndex
rdd.mapPartitionsWithIndex(f).sum()
#6
```

* mapValues(func)：对RDD的元素值执行func，key不变

```python
sc.parallelize([("a", [1, 2, 3]), ("b", 1)]).mapValues(lambda x:len(x)).collect()  
# [('a', 3), ('b', 1)]
```

* flatMap(func)：先map再flat，对RDD的所有元素执行func，然后展平

```python
rdd = sc.parallelize([2, 3, 4])
sorted(rdd.flatMap(lambda x: range(1, x)).collect())
#[1, 1, 1, 2, 2, 3]
sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
#[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
```

* flatMapValues(func)：对RDD的元素值进行func的flatMap操作，key不变，只适用于(key,value)形式的RDD

```python
x = sc.parallelize([("a", ["x", "y", "z"]), ("b", ["p", "r"])])
def f(x): return x
x.flatMapValues(f).collect()
# [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
```

* reduce(func)：对RDD所有元素执行func
* reduceByKey(func)：对RDD所有同一个key的元素执行func
* fold(zerovalue,func)：用法和reduce一样，但会赋给zerovalue初值

```python
from operator import add
sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
# 15
```

* foldByKey(zerovalue,func)：同reduceByKey一样，但是有初值

```python
rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
from operator import add
sorted(rdd.foldByKey(0, add).collect())
# [('a', 2), ('b', 1)]
```

* foreach(func)：对RDD的每个元素都执行func，和map的区别在于：map返回新RDD，是transformer算子；foreach没有返回值，是action算子

```python
sc.parallelize([1, 2, 3, 4, 5]).foreach(lambda x:print(x))
```

* foreachPartition(func)：对RDD的每个分区执行func

```python
sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(lambda x:print(i) for i in x)
```

* join(rdd2)：返回rdd1和rdd2内连接的结果（key相同的那部分），rdd1.join(rdd2)和rdd2.join(rdd1)并不相同

```python
x = sc.parallelize([("a", 1), ("b", 4)])
y = sc.parallelize([("a", 2), ("a", 3)])
sorted(x.join(y).collect())
# [('a', (1, 2)), ('a', (1, 3))]
```

*  leftOuterJoin(rdd2)：左连接

```python
x = sc.parallelize([("a", 1), ("b", 4)])
y = sc.parallelize([("a", 2)])
sorted(x.leftOuterJoin(y).collect())
#[('a', (1, 2)), ('b', (4, None))]
```

* rightOuterJoin(rdd2)：右连接

```python
x = sc.parallelize([("a", 1), ("b", 4)])
y = sc.parallelize([("a", 2)])
sorted(y.rightOuterJoin(x).collect())
#[('a', (2,1)), ('b', (None,4))]
```

* fullOuterJoin(rdd2,partition_num)：类似SQL的表的全连接，两个RDD按key分组聚合，对应key不存在的value为None，且新RDD值的顺序应当一致

```python
x = sc.parallelize([("a", 1), ("b", 4)])
y = sc.parallelize([("a", 2), ("c", 8)])
sorted(x.fullOuterJoin(y).collect())
# [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]
```

* union(rdd2)：rdd1和rdd2合并
* getNumPartitions()：返回此RDD的partition的数目

* glom()：返回分区的RDD

```python
rdd = sc.parallelize([1, 2, 3, 4], 2)
sorted(rdd.glom().collect())
# [[1, 2], [3, 4]]
```

* groupBy(func)：按func分组，返回(key,value)，key从0开始，表示第几组，value是分组结果

```python
rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
result = rdd.groupBy(lambda x: x % 2).collect()
sorted([(x, sorted(y)) for (x, y) in result])
# [(0, [2, 8]), (1, [1, 1, 3, 5])]
```

* groupByKey()：按key分组聚合

```python
rdd = sc.parallelize([("a", 2), ("b", 1), ("a", 1)])
sorted(rdd.groupByKey().mapValues(len).collect())
#[('a', 2), ('b', 1)]
sorted(rdd.groupByKey().mapValues(list).collect())
#[('a', [1, 2]), ('b', [1])]
```

* histogram(bucket)：分桶，bucket是桶数，参数也可以是分好的桶，返回值有两个，第一个是分的桶，第二个是桶内元素的频数

```python
rdd = sc.parallelize(range(51))
rdd.histogram(2) # 分两个桶
#([0, 25, 50], [25, 26]) # 第一个是桶，第二个是频数
rdd.histogram([0, 5, 25, 50]) # 分的桶
#([0, 5, 25, 50], [5, 20, 26])
rdd.histogram([0, 15, 30, 45, 60])  # 桶的边界也可以大于数组边界
#([0, 15, 30, 45, 60], [15, 15, 15, 6])
```

* id()：RDD的id
* setName(name)：给RDD设置名字
* name()：返回rdd的名字
* intersection(rdd2)：rdd1和rdd2的交集，去重显示，会进行shuffle

```python
rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
rdd1.intersection(rdd2).collect()
#[1, 2, 3]
```

* isEmpty()：判断rdd是否为空，返回布尔值
* keys()：返回元组对的键
* values()：返回元组对的值
* lookup(key)：返回key的所有值的列表
* max(key=None)：返回最大值，key指定比较的函数

```python
rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
rdd.max()
#43.0
rdd.max(key=str)
#5.0
```

* mean()：计算平均值
* min(key=None)：返回最小值
* sum()：求和
* stdev()：RDD的标准差
* stats()：返回RDD的均值，方差和计数
* variance()：返回方差
* partitionBy(num)：分成num个partition，返回该副本
* pipe(command,env=None,checkcode=False)：对rdd使用shell的command命令，cheackCode表示是否检查shell的命令返回

```python
sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
#['1', '2', '', '3']
```

* randomSplit(weight,seed=None)：按weight权重随机拆分rdd

```python
rdd = sc.parallelize(range(500), 1)
rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
len(rdd1.collect() + rdd2.collect())
#500
150 < rdd1.count() < 250
#True
250 < rdd2.count() < 350
#True
```

* sample(withReplacement,fraction,seed=None)：采样，withReplacement表示采样是否放回，即是否会采样到同一个数多次；fraction是采样的比例

```python
rdd = sc.parallelize(range(100), 4)
6 <= rdd.sample(False, 0.1, 81).count() <= 14
#True
```

* sampleByKey(withReplcement,fractions,seed=None)：分key采样，withReplacement表示采样是否放回；fractions是各key采样的比例，形式是字典

```python
fractions = {"a": 0.2, "b": 0.1}
rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))
sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())
100 < len(sample["a"]) < 300 and 50 < len(sample["b"]) < 150
#True
```

* takeSample(withReplacement,num,seed=None)：按num固定大小取出，num超过最大长度就全部取出

```python
rdd = sc.parallelize(range(0, 10))
len(rdd.takeSample(True, 20, 1))
20
len(rdd.takeSample(False, 5, 2))
5
len(rdd.takeSample(False, 15, 3))
10
```

* sampleStdev()：采样的样本标准差
* sampleVariance()：采样的样本方差
* sortBy(func,ascending=True)：按func(可以对key也可以对value)排序，默认升序

```python
tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
#[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
#[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
```

* sortByKey(ascending=True,num_partitions,keyfunc)：按keyfunc对key排序

```python
tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
sc.parallelize(tmp).sortByKey().first()
#('1', 3)
sc.parallelize(tmp).sortByKey(True, 1).collect()
#[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
sc.parallelize(tmp).sortByKey(True, 2).collect()
#[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
#[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
```

* subtract(rdd2)：rdd1和rdd2的差集

```python
x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 3)])
y = sc.parallelize([("a", 3), ("c", None)])
sorted(x.subtract(y).collect())
#[('a', 1), ('b', 4), ('b', 5)]
```

* subtractByKey(rdd2)：rdd1减去rdd2的key的差集

```python
x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 2)])
y = sc.parallelize([("a", 3), ("c", None)])
sorted(x.subtractByKey(y).collect())
#[('b', 4), ('b', 5)]
```

* take(num)：取前num个数据元素，适用于数组小的情况，若num超过数组长度就全取

```python
sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
#[2, 3]
sc.parallelize([2, 3, 4, 5, 6]).take(10)
#[2, 3, 4, 5, 6]
```

* takeOrdered(num,key=None)：按指定key规则升序取出num个

```python
sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
#[1, 2, 3, 4, 5, 6]
sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
#[10, 9, 7, 6, 5, 4]
```

* top(num,key=None)：取前几个

```python
sc.parallelize([10, 4, 2, 12, 3]).top(1)
#[12]
sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
#[6, 5]
sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
#[4, 3, 2]
```

* zip(rdd2)：和python的zip用法一致，两两组队

```python
x = sc.parallelize(range(0,5))
y = sc.parallelize(range(1000, 1005))
x.zip(y).collect()
#[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]
```

* zipWithIndex()：和zip一样，只是用RDD自身的索引代替了rdd2，当有多个partition会触发spark作业

```python
sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()
#[('a', 0), ('b', 1), ('c', 2), ('d', 3)]
```

* zipWithUniqueId()：效果和zipWithIndex()一样，但不会触发spark作业

```python
sc.parallelize(["a", "b", "c", "d", "e"], 3).zipWithUniqueId().collect()
#[('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]
```

# StorageLevel

控制rdd的存储等级，主要参数： 

*  useDisk：是否使用硬盘
*  useMemory：是否使用内存
*  useOffHeap： 是否使用堆外内存（可以直接在工作节点的系统内存中开辟空间 ）
* deserialized：是否反序列化，序列化是把对象变为字节，将其永久化，可以反序列化恢复
* replication：在多个节点上复制RDD分区的数目 ，默认1，表示复制1份

常用存储等级：

```python
# 里面的参数对应上面5个
DISK_ONLY = StorageLevel(True, False, False, False, 1)
DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)
MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)
MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)
MEMORY_ONLY = StorageLevel(False, True, False, False, 1)
MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)
OFF_HEAP = StorageLevel(True, True, True, False, 1)
```
